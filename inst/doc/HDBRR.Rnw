%\VignetteIndexEntry{HDBRR-extdoc}
\documentclass[article,shortnames,nojss]{jss}

%Extra packages
\usepackage{amsmath}
\usepackage{amsthm, amssymb,latexsym,amsfonts}
\usepackage{tikz}
\usepackage{subfig}
\usepackage{graphicx}
\usepackage{appendix}
\usepackage{rotating}

\newlength{\RoundedBoxWidth}
\newsavebox{\GrayRoundedBox}
\newenvironment{GrayBox}[1][\dimexpr\textwidth-4.5ex]%
   {\setlength{\RoundedBoxWidth}{\dimexpr#1}
    \begin{lrbox}{\GrayRoundedBox}
       \begin{minipage}{\RoundedBoxWidth}}%
   {   \end{minipage}
    \end{lrbox}
    \begin{center}
    \begin{tikzpicture}%
       \draw node[draw=black,fill=black!10,rounded corners,%
             inner sep=2ex,text width=\RoundedBoxWidth]%
             {\usebox{\GrayRoundedBox}};
    \end{tikzpicture}
    \end{center}}

\newcommand{\y}{\boldsymbol{\mathrm{y}}}
\newcommand{\X}{\boldsymbol{\mathrm{X}}}
\newcommand{\x}{\mathrm{X}}
\newcommand{\yy}{\mathrm{y}}
\newcommand{\U}{\boldsymbol{\mathrm{U}}}
\newcommand{\V}{\boldsymbol{\mathrm{V}}}
\newcommand{\D}{\boldsymbol{\mathrm{D}}}
\newcommand{\p}{\boldsymbol{\mathrm{P}}}
\newcommand{\Q}{\boldsymbol{\mathrm{Q}}}
\newcommand{\R}{\boldsymbol{\mathrm{R}}}
\newcommand{\s}{\boldsymbol{\mathrm{S}}}
\newcommand{\I}{\boldsymbol{\mathrm{I}}}
\newcommand{\B}{\boldsymbol{\beta}}
\newcommand{\si}{\boldsymbol{\Sigma}}
\newcommand{\g}{\boldsymbol{\gamma}}
\newcommand{\post}{\textit{a posteriori }}
\newcommand{\prior}{\textit{a priori }}
\newcommand{\Post}{\textit{A Posteriori }}
\newcommand{\Prior}{\textit{A Priori }}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% declarations for jss.cls %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% almost as usual
\author{Monroy-Castillo B. \\Colegio de Postgraduados$^{*}$ \And \,\,
        P\'erez-Elizald S.$^{*}$ \And \,\,\,\,\,\,\,\,\quad
        P\'erez-Rodr\'iguez P.$^{*}$ \And
        Crossa J.\\CIMMyT}

\title{HDBRR: A Statistical Package for a High Dimension Ridge Regression withouth MCMC}

%% for pretty printing and a nice hypersummary also set:
\Plainauthor{Blanca Monroy-Castillo, Sergio P\'erez-Elizalde, Paulino P\'erez-Rodr\'iguez} %% comma-separated
\Plaintitle{HDBRR: A Statistical Package for High Dimensional Ridge Regression without MCMC} %% without formatting
\Shorttitle{\pkg{HDBRR}: An R-package for Ridge Regression without MCMC} %% a short title (if necessary)

%% an abstract and keywords
\Abstract{
Ridge regression is a useful tool to deal with collinerity in
 the homoscedastic linear regression model, which provide biased estimators
 of the regression parameters with lower variance than the least
 square estimators. Evenmore, when the number
 of predictors ($p$) is much larger than the number of
 observations ($n$), ridge regression give us unique least square estimators by restringing the parametric space to the neighborhood of the origin. From the Bayesian
 point of view ridge regression results of assigning a Gaussian prior on
 the regression parameters and assuming they are conditionally
 independent. However, from both classical and Bayesian approaches the
estimation of parameters is a highly demanding computational task, in the
first one being an optimization problem and in the second one a
high dimensional integration problem usually faced up through
Markov Chain Monte Carlo (MCMC). The main drawback of MCMC is the practical impossibility of checking convergence to the posterior distribution, which is commonly very
slow due to the large number of regression parameters. Here we propose a computational algorithm to obtain posterior estimates of regression parameters, variance components and predictions for the conventional ridge Regression model, based on a reparameterization of the model which allows us to obtain the marginal posterior means and variances by integrating out a nuisance parameter whose marginal posterior is defined on the open interval $(0,1)$.
}
\Keywords{Bayesian Methods, Regression, Variable Selection, Shrinkage, Ridge Regression, MCMC, \proglang{R}}
\Plainkeywords{Bayesian Methods, Regression, Variable Selection, Shrinkage, Ridge Regression, MCMC, R} %% without formatting

%% publication information
%% NOTE: Typically, this can be left commented and will be filled out by the technical editor
%% \Volume{50}
%% \Issue{9}
%% \Month{June}
%% \Year{2012}
%% \Submitdate{2012-06-04}
%% \Acceptdate{2012-06-04}

%% The address of (at least) one author should be given
%% in the following format:
\Address{
  Sergio P\'erez-Elizalde\\
  Socio Econom\'ia Estad\'istica e Inform\'atica \\
  Colegio de Postgraduados, M\'exico\\
  E-mail: \email{sergiop@colpos.mx}\\

  Blanca Monroy-Castillo\\
  Socio Econom\'ia Estad\'istica e Inform\'atica \\
  Colegio de Postgraduados, M\'exico\\
  E-mail: \email{blancamonroy.96@gmail.com}\\

  Paulino P\'erez-Rodr\'iguez\\
  Socio Econom\'ia Estad\'istica e Inform\'atica \\
  Colegio de Postgraduados, M\'exico\\
  E-mail: \email{perpdgo@colpos.mx}\\

}

%% end of declarations %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\SweaveOpts{concordance=TRUE}

%% include your article here, just as usual
%% Note that you should use the \pkg{}, \proglang{} and \code{} commands.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
\label{sec:Intro}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Nowadays most research areas use massive quantities of information generated by the increasingly sophisticated computer equipment; for example, in genomics an increasing amount of data is available as new sequencing technologies appears. A lot of statistical models have been proposed in order to learn valuable information from data; however, even with the simplest models, the statisticians or data scientists have to deal with high dimensional inference problems which require millions of computation tasks. One of such models is the ridge regression, being a useful tool to deal with collinearity in the homoscedastic linear regression model by providing biased estimators of regression parameters with lower variance than the least square estimators. Even more, when the number of predictors ($p$) is much larger than the number of observations ($n$), ridge regression gives a unique least square estimator by restricting the parametric space.

From the Bayesian point of view, ridge regression results of assigning a Gaussian prior on the regression parameters and assuming they are conditionally independent. However, since the Bayesian estimation of parameters is a high dimensional integration problem, it is also a highly demanding computational task which is usually faced up through Markov Chain Monte Carlo (MCMC), in particular Gibbs Sampling because the full posterior conditionals are available in closed form. The most successful MCMC option implemented in the \proglang{R} software is the package \proglang{BGLR}\citep{BGLR} , other non Bayesian \proglang{R} package options are \proglang{penalized}\citep{penal} and \proglang{ridge}\citep{ridge}.

The main drawback of MCMC in high dimensional settings is checking of convergence to the joint posterior distribution, which is commonly very slow due to the large number of regression parameters and the high correlations between successive samples from the conditional posteriors in the Gibbs sampling implementation of MCMC. As \cite{rajaratnam2015} shows for the regression model, meanwhile the MCMC samples yield a good approximation of the posterior means of the regression parameters, their posterior variances and the posterior mean of the residual variance may be underestimated if the simulated Markov chain is not large enough; nevertheless,  the length of the chain is an issue that still being an open research field. In this paper we propose a simple numerical method to estimate posterior means and variances of the parameters in the ridge regression model as a way to abandon the theoretical guarantees of MCMC methods. We use the SVD  and the QR decompositions together with a reparameterization to get closed expressions of the conditional posteriors from where we obtain the marginal posterior means and variances by numerical integration on the open interval $(0,1)$; furthermore, variable selection and prediction are straightforward consequences. The proposed method is implemented in \proglang{R} and allows to work within the big matrix framework by using storing and parallelization packages \proglang{bigstatsr}, \proglang{bigparallelr} and \proglang{parallel}.

\section{Method and Materials}

In the Bayesian approach to inference statistics we formally combine, through the Bayes rule,  prior information and sample data to learn about unknown quantities of interest. The previous to data uncertainty about the parameter of interest  $\theta$ is expressed by the prior distribution, the information about $\theta$ that comes from observed data is incorporated by the likelihood function and by the Bayes formula we obtain the posterior distribution of the parameter given the data (posterior distribution) \citep{Lee}. However, calculating the posterior distribution is not always an easy task because integration is required and, even in low dimensional settings, Monte Carlo or numerical integration methods are needed. In this way, \cite{Gilks} introduced the MCMC (Markov Chain Monte Carlo) which provides a straightforward and intuitive way to both simulate values from an unknown distribution and use those simulated values to perform subsequent analyses \citep{MCMC3}. For more information about MCMC see \cite{MCMC,MCMC2}. In this paper we do not use the sampling based approach to approximate de posterior distribution, instead we focus on a numerical approximation of the posterior of a nuisance parameter to integrate out it and to obtain numerical aproximations of posterior means and variances of regression parameters and predictions.

\subsection{Model}
Consider the linear model
$$
y_i=\mathbf{x}^{\prime}_{i}\boldsymbol{\beta}+\epsilon_i,\quad i=1,\ldots,n.
$$
where $\epsilon_i$, $i=1,\ldots,n$, is a Gaussian error with mean $0$ and variance $\sigma^2$, $\boldsymbol{\beta}\subseteq\mathbb{R}^p$ is a vector of regression coefficients and $\mathbf{x}_{i}$, $i=1,\ldots,n$, is $p$ dimensional vector of observed without error covariates. Moreover, assume that $\mathrm{cov}\left(\epsilon_i,\epsilon_j\right)=0$, $i\neq j$. In matrix form the model is written as
\begin{eqnarray}\label{eqn:model}
\y &=& \X\boldsymbol{\beta} \,\,+\,\, \boldsymbol{\varepsilon},
\end{eqnarray}

where $\B = (\beta_1,\dots, \beta_p)'$ is the vector of \textit{regression parameters}, $\boldsymbol{\varepsilon} = (\varepsilon_1,\dots,\varepsilon_n)^T$ is the vector of random errors distributed as $\mathcal{N}_p(\boldsymbol{0},\sigma^2\I_{nn})$. The $n\times p$ matrix $\X$ is called the \textit{design matrix} and $\y$ is generally referred to as the vector \textit{response variable}. Since the mean of $\y$, $\X\boldsymbol{\beta}$, is a linear combination of the columns of $\X$,  the model in (\ref{eqn:model}) is known as the \textit{linear regression model}.

When the number observations is greater than the number of covariates, $n>p$, the best linear unbiased estimator of $\B$ is $\hat{\B}=(\X'\X)^{-1}\X'\y$. However, when multicollinearity occurs, although the least squares estimators are unbiased, their variances are inflated because $(\X'\X)^{-1}$ tends to be singular. Another scenario where the obtention of the least squares estimator is an ill-posed problem occurs when $p>>n$, which implies that $\hat{\B}$ is not unique. In both cases some sort of restriction of the parametric space or penalization is needed in order to have unique estimators with lower variance. By adding a degree of bias to the regression estimates, ridge regression gives an unique estimator of $\B$ with variance lower than the least squares estimator variance.

\subsection{Ridge Regression}
\cite{Hoerld} and \cite{Hoerle} first suggested that to control the inflation and general instability associated with the least squares estimates we may use the same Tikhonov regularization for all the regression parameters, which gives the ridge estimator:
\begin{eqnarray}\label{eq:ridge}
\hat{\B}^* &=& \left(\X'\X+k\I\right)^{-1}(\X'\y); \quad \quad k>0,
\end{eqnarray}
where $k$ is the ridge penalty parameter. Large values of $k$ tend to reduce the magnitude of the estimated regression coefficients, leading to fewer effective model parameters \cite{Cannon}. See \citet{Hoerla,Hoerlb,Hoerlc,RR,Alheety,waheed} for more about ridge regression.

\subsection{Bayesian inference for ridge regression}
In Bayesian inference all the uncertainty about the unknown parameters $\left(\B,\sigma^2\right)$ is described by the join posterior distribution, which is obtained, through the Bayes rule, as the likelihood function, the joint density of $\y$ seen as a function of the parameters, times the prior. The prior is a probability density function we use to measure the uncertainty about $\left(\B,\sigma^2\right)$ before any data has been observed.

From model in (\ref{eqn:model}) the likelihood function for the parameters $\left(\B,\sigma^2\right)$ is given by

\begin{eqnarray}
L (\boldsymbol{\beta}, \sigma^2\,\, |\,\, \y) &\propto& \left(\frac{1}{\sigma^2}\right)^{n/2} \exp\left\{-\frac{1}{2\sigma^2}(\y-\X\boldsymbol{\beta})^T(\y-\X\boldsymbol{\beta})\right\}.\label{eq:likfun}
\end{eqnarray}

If the prior for each $\beta_j$, $j=1,\ldots,p$, is Gaussian with mean $0$ and variance $\sigma_\beta^2$ and prior independence of $\B$ and $\sigma^2$ is assumed then the joint prior is of the form
\begin{eqnarray}
\pi(\B, \sigma^2,\sigma^2_{\B}) &=& \pi\left(\B\, |\, \sigma^2_{\B}\right)\,\pi(\sigma^2)\,\pi(\sigma^2_{\B}),\label{eq:prior}
\end{eqnarray}
where,
\begin{eqnarray*}
\pi\left(\B\, |\, \sigma^2_{\B}\right) &=& \mathcal{N}_p\left(\B\,|\, \boldsymbol{0}, \sigma^2_{\B}\I_{p}\right)
\end{eqnarray*}
and for the variance parameters we may assign conjugate priors \citep{prior, prior2}; that is, the inverse gamma distributions
\begin{eqnarray*}
\pi\left(\sigma^2\right) &=& \mathcal{IG}\left(\sigma^2\left | \frac{n_0}{2},\frac{n_0s_0^2}{2}\right.\right) \\
\pi\left(\sigma^2_{\B}\right) &=& \mathcal{IG}\left(\sigma^2_{\B}\left | \frac{p_0}{2},\frac{p_0d_0^2}{2}\right.\right),
\end{eqnarray*}
where $n_0,s_0^2,p_0$ and $d_0^2$ are known hyperparameters.The prior independence of the elementos of $\B$ given $\sigma_\beta^2$ implies that marginally the distribution of $\B$ is multivariate $\emph{t}$ with $p_0$ degrees of freedom. Then, this hierarchical structure implies that regression parameters are not independent to each other, which seems to be an appropiated structure in presence of colineallity or when $p>>n$. The prior variance of the elements of $\B$ has also an interpretation in terms of regularization: for fixed $\sigma^2$, as $\sigma_\beta^2$ tends to $0$ the shrinkage to the prior mean increases, which means that large values of the parameters are penalized.

Due to the problem of estimation of the ridge regression parameters is ill-posed, prior elicitation is a critical step in Bayesian inference since the posterior is too sensitive to the assignation of values to the hyperparameters in the prior of $\B$. Here we use an approach closed to those proposed by \cite{Yongtao} and \cite{BGLR} to model the initial knowledge of $\B, \sigma^2$ and $ \sigma^2_{\beta}$. Thus, the prior expected values of $\sigma^2$ and $ \sigma^2_{\beta}$ are

\begin{eqnarray*}
\text{E}\left[\sigma^2 \left|  \frac{n_0}{2},\frac{n_0s_0^2}{2} \right.\right] &=& \frac{n_0s_0^2}{n_0-2}, \quad \quad \quad n_0>2.\\
\text{Var}\left[\sigma^2 \left|  \frac{n_0}{2},\frac{n_0s_0^2}{2} \right.\right] &=& \frac{n_0^2s_0^4}{(n_0-2)^2(2n_0-8)}, \quad \quad \quad n_0>4.
\end{eqnarray*}
Therefore, in order to have prior moments of first and second order finite for the variance components, the value of $n_0$ and $p_0$ should be at least 5. Then,we use 5 as the default value for $n_0$ and $p_0$ in the \proglang{HDBRR} package, but flat priors could be obtained as $n_0$ and $p_0$ tend to $0$. To assign values to $s_0^2$ and $d_0^2$ we will use the prior expected value of the proportion of variance explained (PVE) by the model with respect to the residual variance, that is given by
\begin{eqnarray*}
\text{PVE} &=& \frac{1}{n}\sum_{i=1}^n \frac{(\boldsymbol{x}_i'\boldsymbol{\beta})^2}{\sigma^2} \,\,=\,\,\frac{1}{n\sigma^2} \sum_{i=1}^n(\sum_{j=1}^p x_{ij}\beta_j)^2.
\end{eqnarray*}
Then, by noting that
\begin{eqnarray*}
\text{E}\left[\left(\sum_{j=1}^p x_{ij}\beta_j\right)^2\big\vert\sigma^2,\sigma^2_{\beta}\right]&=&\text{Var}\left[\sum_{j=1}^p x_{ij}\beta_j\big\vert\sigma^2,\sigma^2_{\beta}\right]=\sigma^2_{\beta}\sum_{j=1}^p x_{ij}^2
\end{eqnarray*}
we have that
\begin{eqnarray*}
\text{E}[\text{PVE}] &=& \text{E}[\text{E}[\text{PVE}\big\vert\sigma^2,\sigma^2_{\beta}]] \,\, = \,\, \frac{1}{s_0^2}\frac{p_0d_0^2}{p_0-1}\sum_{i=1}^n\sum_{j=1}^p\frac{x_{ij}^2}{n}.
\end{eqnarray*}
Let
\begin{eqnarray*}
\text{h} &=& \frac{\text{E}[\text{PVE}]}{1+\text{E}[\text{PVE}]} \,\,=\,\, \frac{p_0d_0^2\sum_{i=1}^n\sum_{j=1}^p\frac{x_{ij}^2}{n}}{s_0^2(p_0-1)+p_0d_0^2\sum_{i=1}^n\sum_{j=1}^p\frac{x_{ij}^2}{n}}.
\end{eqnarray*}
the proportion of the total prior variance explained by the model. From this,
\begin{eqnarray*}
\frac{h}{1-h} &=& \frac{p_0d_0^2\sum_{i=1}^n\sum_{j=1}^p\frac{x_{ij}^2}{n}}{s_0^2(p_0-1)},
\end{eqnarray*}
therefore
\begin{eqnarray*}
d_0^2 &=& \left(\frac{h}{1-h}\right)\frac{s_0^2(p_0-1)}{p_0\sum_{i=1}^n\sum_{j=1}^p\frac{x_{ij}^2}{n}}.
\end{eqnarray*}
The value $s_0^2$ may be interpreted as a prior guess about the residual variance.

In particular, when each covariate and the response have been centered about its sample mean
\begin{eqnarray*}
d_0^2 &=& \left(\frac{h}{1-h}\right)\frac{s_0^2(p_0-1)}{p_0\sum_{j=1}^ps_j^2};
\end{eqnarray*}
moreover, if the all the data are standardized and $s_0^2=1$ we have that $d_0^2=\left(\frac{h}{1-h}\right)\frac{(p_0-1)}{p_0p}$ which approaches to $1/p$ when $h=0.5$ and $p_0$ is large. Also note that as $h$ tends to $1$ the prior for $\sigma_{\beta}^2$ becomes flat but proper distribution.

Once the prior distribution has been assigned and the likelihood function defined, then  the posterior distribution of the regression parameters is derived in what follows.
\subsection{Posterior Distribution}
By the Baye's Rule, the joint posterior is obtained as the product of likelihood function in (\ref{eq:likfun}) and the prior in (\ref{eq:prior}). Thus,
\begin{eqnarray*}
\pi\left(\B,\,\sigma^2,\,\sigma^2_{\B}\mid\y\right) &\propto& L\left(\B,\, \sigma^2\,| \y\right)\, \pi(\B, \sigma^2,\sigma^2_{\B}),
\end{eqnarray*}
Now, cosiderer the transformations $v \,=\, 1/\sigma^2\,+\, 1/\sigma^2_{\B}$ and $u \,=\, \sigma^2/\left(\sigma^2 +\sigma^2_{\B}\right)$; then, the joint posterior distribution of $(\B, \, u,\, v)$ is given by
\begin{eqnarray}\label{eqn:posterior}
\pi(\B,\,u,\,v\mid\y) &\propto& v^{\frac{n+n_0+p+p_0}{2}-1}u^{\frac{p+p_0}{2}-1}(1-u)^{\frac{n+n_0}{2}-1} \\\nonumber
& & \times \exp\left\{-\frac{v(1-u)}{2}\left[\left(\B-\hat{\B}(u)\right)'\si_n^{-1}(u)\left(\B-\hat{\B}(u)\right)\,+\, SSE(u)\,+\, n_0s_0^2\right] \right\}\\\nonumber
& &\quad \times \exp\left\{-\frac{uv}{2}\left(\hat{\B}(u)\hat{\B}(u)\,+\,p_0d_0^2\right)\right\},
\end{eqnarray}
where $\si_n(u) \,=\, \left(\X'\X\,+\, \frac{u}{1-u}\I\right)^{-1}$, $\hat{\B}(u) \,=\, \si_n(u)\X'\y$,  $\hat{\y}(u) \,= \, \X\hat{\B}(u)$ and $SSE(u) \,=\, \left(\y-\hat{\y}(u)\right)'\left(\y-\hat{\y}(u)\right)$.  From (\ref{eqn:posterior}), the full conditional posterior of $\B$ is
\begin{eqnarray}
\pi(\B \,|\, u,\,v,\,\y) &=& \mathcal{N}_p \left(\B \,\left |\, \hat{\B}(u),\, \frac{1}{v(1-u)}\si_n(u)\right.\right).
\end{eqnarray}
Now, let $S(u) \,=\, (1-u) \, (SSE(u)\,+\, n_0s_0^2) \,+\, u(\hat{\B}'(u)\hat{\B}(u) \,+\, p_0d_0^2)$ and by the definition of conditional distribution of $\B$ given $\left(u,v\right)$, we have that
\begin{eqnarray}
\pi(v \, |\, u,\, \y) &=& \frac{\pi(\B,\, v \,|\, u,\,\y)}{\pi(\B \,|\, u,\,v,\,\y)} \quad = \quad \mathcal{G} \left(v \, \left | \frac{n+n_0+p_0}{2},\, \frac{S(u)}{2}\right.\right),
\end{eqnarray}
where $\pi(\B,\, v \,|\, u,\,\y)$, from (\ref{eqn:posterior}), is a Normal-Gamma density. It follows that, the posterior distribution of $\B$ given $u$ is $t$ with $\nu \,= \, n + n_0 + p_0$ degrees of freedom, mean
\begin{eqnarray*}
\text{E} [\B \,|\, u, \, \y] &=& \hat{\B}(u)
\end{eqnarray*}
and variance
\begin{eqnarray*}
\text{V} [\B\,|\, u,\, \y] &=& \frac{S(u)}{(\nu-2)(1-u)}\si_n(u), \quad \quad \quad \quad \nu-2 > 0.
\end{eqnarray*}
Finally, the marginal posterior of $u$ is obtained as
\begin{eqnarray}\label{eq:posterior}
\pi(u \,|\, \y) &=& \frac{\pi(u, \, v \, |\,  \y)}{\pi(v \,|\, u, \, \y)} \quad = \quad \frac{\pi(\B, \,u, \, v\, | \, \y )/ \pi(\B\,|\, u,\, v, \, \y)}{\pi(v\,|\, u,\, \y)}\nonumber \\
&\propto& u^{\frac{p+p_0}{2}-1}(1-u)^{\frac{n-p+n_0}{2}-1}\left|\si_n(u)\right|^{1/2} \nonumber \\
& & \times \left[(1-u)(SSE(u)+n_0s_0^2)\,+\, u\,\left(\hat{\B}'(u)\hat{\B}(u)+p_0d_0^2\right)\right] ^{-\frac{n+n_0+p_0}{2}}, u\in\left(0,1\right)
\end{eqnarray}
It is important to point out that the marginal moments $\pi(\B\mid\y)$ can be obtained by theorem of total expectation. I such a way, the unconditional posterior mean and variance of $\B$ are, respectively,
\begin{eqnarray*}
\text{E} [\B \,|\, \y]=\text{E} \left[\text{E} [\B \,|\, u, \, \y]\right] &=& \int_0^1\hat{\B}(u)\pi(u \,|\, \y)du
\end{eqnarray*}
and
\begin{eqnarray*}
\text{V} [\B \,|\, \y]&=&\text{E} \left[\text{V} [\B \,|\, u, \, \y]\right]+\text{V} \left[\text{E} [\B \,|\, u, \, \y]\right]\\
&=&\int_0^1\left(\frac{S(u)}{(\nu-2)(1-u)}\si_n(u)+\left(\hat{\B}(u)-\text{E} [\B \,|\, \y]\right)^2\right)\pi(u \,|\, \y)du.
\end{eqnarray*}
Both integrals above may be evaluated numerically with accurate precision in most cases.

\subsubsection{Marginal posterior distributions of variance components}
The marginal distributions of $\sigma^2$ and $\sigma^2_{\B}$ are obtained from the joint distribution, $\pi(u,\, v \,|\,\y) \,=\, \pi(v\,|\, u, \y)\pi(u\,|\,\y)$; that is, using the change of variable formula,
\begin{eqnarray*}
\pi(\sigma^2,\,\sigma^2_{\B}\, |\, \y) &=& \pi(v(\sigma^2,\,\sigma^2_{\B})\,|\, u\left(\sigma^2,\,\sigma^2_{\B}\right),\,\y)\, \pi \left(u\left(\sigma^2,\,\sigma^2_{\B}\right)\, \mid\y\right)\left|\frac{\partial(u,v)}{\partial\left(\sigma^2,\sigma^2_{\B}\right)}\right.
\end{eqnarray*}
However, the marginals can not be obtained in closed form, so numerical or Monte Carlo integration over $\mathbb{R}^+$ is needed. However, if only points estimates are needed, it is possible to get them using one dimensional integration over the interval $(0,1)$. For example, the Bayesian estimator under square loss of the ridge parameter $\lambda \,=\, \sigma^2/\sigma^2_{\B} \,=\, u/(1-u)$ is given by
\begin{eqnarray*}
\hat{\lambda}  &=&\text{E}[\lambda\mid\y] =\int_0^1\frac{u}{1-u} \, \pi(u \,|\, \y)\,\text{d}u.
\end{eqnarray*}
In the same way, the posterior means of $\sigma^2_{\B}$ and $\sigma^2$ are
\begin{eqnarray*}
\text{E}\left[\sigma^2_{\B} \,|\, \text{y}\right] &=& \text{E}\left[\left. \frac{1}{uv}\,\right| \text{y}\right] \quad = \quad \text{E}\left[\frac{1}{u}\, \text{E}\left[\left. \frac{1}{v}\right | u, \, \text{y}\right]\right] \\
&=& \frac{1}{n+n_0+p_0-1}  \int_0^1 \frac{S(u)}{u}\, \pi(u\,|\, \y) \,\text{d}u.
\end{eqnarray*}
and
\begin{eqnarray*}
\text{E}[\sigma^2\,|\, \text{y}] &=& \text{E} \left[\left. \frac{1}{v(1-u)}\,\right |\, \text{y} \right] \quad = \quad \frac{1}{n+n_0+p_0-1}\,\int_0^1 \frac{S(u)}{1-u}\,\pi(u\,|\, \y)\, \text{d}u.
\end{eqnarray*}
\subsection{Variable Selection}
Suppose that the prior density for $\B$ is such that,
\begin{eqnarray*}
\pi\left(\beta_j \,|\,\sigma_\beta^2,\, \gamma_j\right) &=& (1-\gamma_j)\text{N}\left(\beta_j \,|\, 0,\sigma^2_{\beta}\right)\, +\, \gamma_j \text{N} \left(\beta_j \,|\, 0, c_j^2\sigma^2_{\beta}\right), \quad j \,=\, 1,\dots,p.
\end{eqnarray*}
Where $\gamma_j \stackrel{iid}{\sim} \text{Bernoulli}(\phi_j)$ is an indicator variable which is $\gamma_j=1$ if the $j-th$ predictor variable is included in the model, in other case $\gamma_j = 0$. To use this hierarchical mixture setup for variable selection, the hyperparameters $\sigma^2_{\beta}$ and $c^2\sigma^2_{\beta}$ are set ``small and large", respectively, so that $\mathcal{N}(0,\sigma^2_{\beta})$ is concentrated around $0$ and $\mathcal{N}(0,c_j^2\sigma^2_{\beta})$ is diffuse as in Figure \ref{fig:delta}.
\begin{figure}[htbp]
\centering
\includegraphics[scale=0.65]{1}
\caption{$\mathcal{N}(0,\sigma^2_{\beta})$ and $\mathcal{N}(0,c_j^2\sigma^2_{\beta})$ densities. Intersection at $\delta_{i}$}
\label{fig:delta}
\end{figure}

If the data supports $\gamma_j = 0$ over $\gamma_j = 1$, then $\beta_j$ is probably small enough so that $\mathrm{X}_j$ will not be needed in the model. Suppose a value $\delta_{j} > 0$ such that if $|\beta_j|<\delta_{j}$ it would be preferable to exclude $\mathrm{X}_j$. The parameter $\delta_{j}$ should be chosen so that the posterior probability $\Pr(\gamma_j=1\mid\y)$ must be higher for those values of $\beta_j$ such that $|\beta_j|>\delta_{j}$ than for those in the neighborhood of $0$. Before any data has been observed, $\delta_{j}$ may be fixed by choosing $\sigma^2_{\beta}$ and $c_j^2\sigma^2_{\beta}$ such that the pdf $\pi\left(\beta_j\,|\, \gamma_{j} = 0\right)\,=\, \mathcal{N}(\beta_j\mid 0,\sigma^2_{\beta})$ is larger than the pdf $\pi\left(\beta_j\,|\, \gamma_{j} = 1\right)\,=\, \mathcal{N}(\beta_j\mid 0,c_j^2\sigma^2_{\beta})$ on the interval $\left(-\delta_{j\gamma},\delta_{j\gamma}\right)$ (see Figure 1). This condition is satisfied for any $\sigma_{\beta}$ and $c_j$ such that
\begin{eqnarray}
\frac{\log\left({\frac{c_j^2\sigma^2_{\beta}}{\sigma^2_{\beta}}}\right)}{\frac{1}{\sigma^2_{\beta}}-\frac{1}{c_j^2\sigma^2_{\beta}}} &\leq& \delta_{j}^2.
\end{eqnarray}
Hence,
\begin{eqnarray*}
\delta_{j} &=& \sqrt{\frac{2c_j^2\sigma^2_{\beta}\log{(c_j)}}{c_j^2-1}},\quad c_j>1.
\end{eqnarray*}
For the selection of $c_j$, note that it is equal to the ratio of $\pi\left(\beta_j=0\,|\, \gamma_{j} = 0\right)$ and $\pi\left(\beta_j=0\,|\, \gamma_{j} = 1\right)$; thus, $c_j$ may be interpreted as the prior odds that $X_j$ should be excluded of the model if $\beta_j$ is to small. Further explanation about the selection of $c_j$ and $\delta_j$ can be found in \cite{Mc}. In what follows, we will assume that, for $j=1,\ldots,p$,  $\phi_j=\phi$ and $c_j=c$, which implies that $\delta_j=\delta$.  In this way, the variable selection procedure is suitable for covariates in the same scale.

The joint posterior distribution of $\g = (\gamma_1,\dots,\gamma_p)$ is given by

$$
\pi(\g \,|\, \y)\,=\, \pi(\y\,|\, \g)\pi(\g)/\pi(\y)
$$
where the marginal model given $\gamma$ in terms of the joint posterior of $\left(\B,\sigma^2,\sigma^2_{\beta}\right)$ is
\begin{eqnarray*}
\pi\left(\y \,|\, \g\right) &=& \frac{L\left(\B, \sigma^2\,|\, \y\right)\pi\left(\B\,|\, \sigma^2_{\beta}, \g\right)\pi\left(\sigma^2\right)\pi\left(\sigma^2_{\beta}\right)}{\pi\left(\B,\sigma^2,\sigma^2_{\beta}\,|\, \g,\y\right)}.
\end{eqnarray*}
Then,
\begin{eqnarray}\label{eqn:postga}\nonumber
\pi(\g\,|\, \y) &=& \quad \frac{\pi(\g)\,L\left(\B,\sigma^2\,|\, \y\right)\pi\left(\B\,|\, \sigma^2_{\beta},\g \right)\,\pi\left(\sigma^2\right)\pi\left(\sigma^2_{\beta}\right)}{\pi(\y)\,\pi\left(\B,\sigma^2,\sigma^2_{\beta}\,|\, \g,\y\right)} \\
&\propto& \frac{\pi(\g)\pi\left(\B \left | \sigma^2_{\beta},\g\right.\right)}{\pi\left(\left. \B,\sigma^2,\sigma^2_{\B}\,\right|\, \g,\y\right)}
\end{eqnarray}
where it should be noted that $\pi\left(\left. \B,\sigma^2,\sigma^2_{\B}\,\right|\, \g\,=\, \boldsymbol{0},\y\right)$ is the joint posterior given in (\ref{eqn:posterior}). Since,
\begin{eqnarray*}
\pi(\g\,|\, \y) &=& \pi\left(\gamma_j\,|\, \g_{-j}, \y \right)\pi\left(\g_{-j}\,|\, \y\right),
\end{eqnarray*}
where $\boldsymbol{\theta}_{-j}$ is the subvector composed by all the elements of $\boldsymbol{\theta}$ except the $j$-th element $\theta_j$. Then,  it follows from (\ref{eqn:postga}) and prior independence that
\begin{eqnarray}\label{eqn:selection}
\pi\left(\gamma_j\,|\, \g_{-j},\y\right) &=& \frac{\pi(\g\,|\, \y)}{\pi\left(\g_{-j}\,|\,\y\right)} \quad \propto \quad \frac{\pi\left(\gamma_j\right)\pi\left(\beta_j \mid \sigma^2_{\beta},\gamma_j \right)}{\pi\left(\g_{-j}\,|\, \y\right)\pi\left(\B,\sigma^2,\sigma^2_{\beta}\,|\, \g,\y \right)} \nonumber \\ &\propto& \frac{\phi^{\gamma_j}(1-\phi)^{1-\gamma_j}\pi\left(\beta_j \,|\, \sigma^2_{\beta},\gamma_j\right)}{\pi\left(\B,\sigma^2,\sigma^2_{\beta}\,|\, \g,\y\right)}.\nonumber
\end{eqnarray}
Expressing the right hand side of the result above in terms of $(\B,\,u,\,v)$ we have the following:
\begin{eqnarray*}
\pi(\gamma_j\,|\, \g_{-j},\y) &\propto& \frac{\phi^{\gamma_j}(1-\phi)^{1-\gamma_j}\pi\left(\beta_j\,|\, u,v,\gamma_j\right)}{\pi\left(\B,u,v\,|\, \g,\y\right)} \\ &\propto& \frac{\phi^{\gamma_j}(1-\phi)^{1-\gamma_j}\pi\left(\beta_j \,|\,u,v,\gamma_j\right)}{\pi\left(\beta_j \,|\, \B_{-j},u,v,\g,\y\right)\pi\left(\B_{-j}\,|\, u,v,\g,\y\right)\pi(v\,|\, u,\g,\y)\pi(u\,|\, \g,\y)}\\ &\propto& \frac{\phi^{\gamma_j}(1-\phi)^{1-\gamma_j}\pi\left(\beta_j \,|\,u,v,\gamma_j\right)}{\pi\left(\beta_j \,|\, \B_{-j},u,v,\g,\y\right)\pi(v\,|\, u,\g,\y)\pi(u\,|\, \g,\y)}.
\end{eqnarray*}
Then, the conditional posterior probability of exclusion of the variable $X_j$ is
\begin{eqnarray}\label{eqn:sel2}
\text{Pr}\left(\gamma_j = 0 \,|\, \g_{-j} \,=\, \boldsymbol{0},\y\right)&=& \pi\left(\gamma_j \,=\,0 \,|\, \g_{-j} \,=\, \boldsymbol{0},\y\right)\\ &\propto& \frac{(1-\phi)\pi\left(\beta_j\,|\, u,v,\gamma_j = 0\right)}{\pi\left(\beta_j \,|\, \B_{-j},u,v,\g=\boldsymbol{0},\y\right)\pi(v\,|\, u,\g=\boldsymbol{0},\y)\,\pi(u\,|\, \g=\boldsymbol{0},\y)}\nonumber \\
&=&C_j \frac{(1-\phi)\pi\left(\beta_j\,|\, u,v,\gamma_j = 0\right)}{\pi\left(\beta_j \,|\, \B_{-j},u,v,\g=\boldsymbol{0},\y\right)}. \nonumber
\end{eqnarray}
Note that probability exclusion (\ref{eqn:sel2}) does not depend on the values of $\B,\,u$ and $v$  and should be equal to one when the conditional posterior mean of $\beta_j$ is equal to the prior mean and $\beta_j=0$.  Moreover, since the prior and the conditional posterior are both normal, it can be verified  that he proportionality constant is $C_j= (1-\phi)^{-1}\sqrt{\sum_{i = 1}^n \frac{1-u}{u}x_{ij}^2+1}$. Thus, the probability of the $j$-th predictor to be included in the model given that the others have been excluded is:
\begin{eqnarray}\label{eqn:phat}
\hat{p}_j=\text{Pr}\left(\gamma_j \,=\, 1 \,|\, \g_{-j}\,=\, \boldsymbol{0},\y\right) &=&1 - \text{Pr}\left(\gamma_j = 0 \,|\, \g_{-j}=\boldsymbol{0},\y\right) \nonumber \\ &=& 1 - \exp\left\{-\frac{\tilde{u}\tilde{v}}{2}\tilde{\beta}_j^2-\frac{(1-\tilde{u})\tilde{v}}{2V_j}\left(\tilde{\beta}_j-E_j\right)^2\right\}
\end{eqnarray}
where, $V_j=\left(\sum_{i = 1}^n x_{ij}^2+\frac{\tilde{u}}{1-\tilde{u}}\right)^{-1}$ and $E_j=V_j\x_{j}^{\prime}\left(\y-\X_{-j}\tilde{\B}_{-j}\right)$ are the conditional posterior variance and mean of $\beta_j$. The quantities $\tilde{u}$,$\tilde{v}$ and $\tilde{\B}$ are arbitrary values $u$,$v$ and $\B$, but with high posterior density. From(\ref{eqn:phat}) we have that the conditional Bayes factor in favour of including the covariate $\x_j$ in the model is
$$
BF_j=\frac{(1-\phi)\hat{p}_j}{(1-\hat{p}_j)\phi}=\frac{(1-\phi)}{\phi}\left(\exp\left\{\frac{\tilde{v}(1-\tilde{u})}{2}\left[\frac{\tilde{u}}{(1-\tilde{u})}\tilde{\beta}_j^2+\frac{1}{V_j}\left(\tilde{\beta}_j-E_j\right)^2\right]\right\}-1\right)
$$

\subsection{Posterior Computation}
The main feature of the parameterization  proposed above is that it enable to use standard matrix algebra to speed up the computation of the posterior distributions without using sampling techniques. This helps to significantly reduce computation time, avoiding slow sampling methods. In fact, once the posterior distribution of  $u$ is calculated all the estimations of interest are almost automatically available.
\subsubsection{Posterior computation through SVD decomposition}
Let $\U\D\V'$ the full singular value decomposition (SVD) of the matrix of covariates $\X$ in the linear model (\ref{eqn:model}). Note that $\V=[\V_1,\V_2]$, where $\V_1$ and $\V_2$ are orthonormals and $\D= [\s,\boldsymbol{0}]$ is a rectangular diagonal matrix, where $\s$ is the diagonal matrix of size  $n \times n$ of positive singular values, $s_1 \geq \dots \geq s_n$ of $\X$ and the last $p-n$ columns are all vectors of zeros. \\ Hence
\begin{eqnarray*}
\si_n(u) &=& \left(\X'\X+\frac{u}{1-u}\I_p\right)^{-1}\\
&=& \left(\V\D'\D\V'+\frac{u}{1-u}\V\V'\right)^{-1}\\
&=& \V\left(\begin{bmatrix}  \boldsymbol{\Lambda} & \boldsymbol{0}_{n,p-n} \\ \boldsymbol{0}_{p-n,n} & \boldsymbol{0}_{p-n,p-n}\end{bmatrix}+\frac{u}{1-u}\I_p\right)^{-1}\V' \\
&=& \V\begin{bmatrix} \boldsymbol{\Lambda}+\frac{u}{1-u}\I_n & \boldsymbol{0}_{n,p-n} \\ \boldsymbol{0}_{p-n,n} & \frac{u}{1-u}\I_{p-n} \end{bmatrix}^{-1}\V',
\end{eqnarray*}
where $\boldsymbol{\Lambda} = \s'\s = \text{diag}(\lambda_1,\dots,\lambda_n)$ is diagonal matrix whose elements are the eigenvalues of $\X\X'$. \\ Similarly,
\begin{eqnarray*}
\hat{\B}_u &=& \si_u\X'\y \\
&=& \V \begin{bmatrix}\boldsymbol{\Lambda} + \frac{u}{1-u}\I_n & \boldsymbol{0}_{n,p-n} \\ \boldsymbol{0}_{p-n,n} & \frac{u}{1-u}\I_{p-n}  \end{bmatrix}^{-1}\V'\V\D'\U'\y \\
&=& \V_1\left(\boldsymbol{\Lambda} + \frac{u}{1-u}\I_n\right)^{-1}\s\U'\y.
\end{eqnarray*}
And
\begin{eqnarray*}
\hat{\y}(u) &=& \X\hat{\B}(u) \\
&=& \U\D\V' \V_1\left(\boldsymbol{\Lambda} + \frac{u}{1-u}\I_n\right)^{-1}\s\U'\y \\
&=& \U\s \left(\boldsymbol{\Lambda} + \frac{u}{1-u}\I_n\right)^{-1} \s'\U'\y\\
&=& \U\boldsymbol{\mathrm{P}}\U'\y,
\end{eqnarray*}
where $\boldsymbol{\mathrm{P}}$ is a diagonal matrix with $\boldsymbol{\mathrm{P}}_{jj} = \frac{(1-u)\lambda_j}{(1-u)\lambda_j+u}$, $j=1,\dots,n$. \\ Thus, for $p>n$, substituting in (\ref{eq:posterior}) the covariance matrix by its SVD decomposition we have,\\
\begin{eqnarray}\label{eq:posterior1}
\pi(u \,|\, \y) &\propto& u^{\frac{p_0+p}{2}-1}(1-u)^{\frac{n_0+n-p}{2}-1} |\X'\X+\frac{u}{1-u}\I|^{-1/2} [(1-u)(SSE(u)+n_0s_0^2)]^{-\frac{n+n_0+p_0}{2}} \nonumber \nonumber \\ && \times\left[1+\frac{u}{1-u}\frac{\hat{\boldsymbol{\beta}}'(u)\hat{\boldsymbol{\beta}}(u)+p_0d_0^2}{SSE(u)+n_0s_0^2}\right]^{-\frac{n+n_0+p_0}{2}} \nonumber \\ &\propto& u^{\frac{p_0+p}{2}-1}(1-u)^{\frac{n_0+n-p}{2}-1}\begin{vmatrix} \boldsymbol{\Lambda}+\frac{u}{1-u}\I_n &  \boldsymbol{0}_{n,p-n} \nonumber \\ \boldsymbol{0}_{p-n,p} &  \frac{u}{1-u}\I_{p-n}\end{vmatrix}^{-1/2} \\ && \times \, [(1-u)(SSE(u)+n_0s_0^2)]^{-\frac{n+n_0+p_0}{2}}\left[1+\frac{u}{1-u}\frac{\hat{\boldsymbol{\beta}}'(u)\hat{\boldsymbol{\beta}}(u)+p_0d_0^2}{SSE(u)+n_0s_0^2}\right]^{-\frac{n+n_0+p_0}{2}} \nonumber \\
&\propto&  u^{\frac{p+p_0}{2}-1}(1-u)^{\frac{n+n_0}{2}-1}u^{\frac{n-p}{2}}\left(\prod_{j=1}^n(1-u)\lambda_j+u\right)^{-\frac{1}{2}} \\ & & \times  [(1-u)(SSE(u)+n_0s_0^2)]^{-\frac{n+n_0+p_0}{2}}  \, \left[1+\frac{u}{1-u}\frac{\hat{\boldsymbol{\beta}}'(u)\hat{\boldsymbol{\beta}}(u)+p_0d_0^2}{SSE(u)+n_0s_0^2}\right]^{-\frac{n+n_0+p_0}{2}},\nonumber
\end{eqnarray}
where
\begin{eqnarray}
SSE(u) &=& (\y-\hat{\y}(u))'(\y-\hat{\y}(u)) \nonumber\\
&=& (\y-\U\boldsymbol{\mathrm{P}}\U'\y)' (\y-\U\boldsymbol{\mathrm{P}}\U'\y)\\
&=& \y'\U(\I-\boldsymbol{\mathrm{P}})^2\U'\y. \nonumber
\end{eqnarray}

In general,the marginal posterior of $u$ is
\begin{eqnarray}
\pi(u \,|\, \y) &\propto&u^{\frac{p+p_0}{2}-1}(1-u)^{\frac{n+n_0}{2}-1}u^{\frac{n-p}{2}I(p>n)}\left(\prod_{j=1}^{\min(n,p)}(1-u)\lambda_j+u\right)^{-\frac{1}{2}} \\\nonumber
& & \times  [(1-u)(SSE(u)+n_0s_0^2)]^{-\frac{n+n_0+p_0}{2}}  \, \left[1+\frac{u}{1-u}\frac{\hat{\boldsymbol{\beta}}'(u)\hat{\boldsymbol{\beta}}(u)+p_0d_0^2}{SSE(u)+n_0s_0^2}\right]^{-\frac{n+n_0+p_0}{2}}
\end{eqnarray}
Also note that
\begin{eqnarray*}
\text{Cov}\left(\hat{\y}(u),\y\,|\, u\right) &=& \text{Cov}\left(\U\boldsymbol{\mathrm{P}}\U'\y,\y\,|\, u\right)\,\, =\,\, \U\boldsymbol{\mathrm{P}}\U'\text{Cov}(\y)\,\, = \,\, \sigma^2\U\boldsymbol{\mathrm{P}}\U'
\end{eqnarray*}
and
\begin{eqnarray*}
\text{Var}\left(\hat{\y}(u)\,|\, u\right) &=& \sigma^2 \U\boldsymbol{\mathrm{P}}\U'	,
\end{eqnarray*}
hence
\begin{eqnarray}\label{eq:cor}
\text{cor}\left(\hat{\mathrm{y}}_i,\mathrm{y}_i\right) &=& \int_0^1 \frac{\sum_{j=1}^ru_{i,j}^2\boldsymbol{\mathrm{P}}_{jj}}{\sqrt{\sum_{j=1}^ru_{i,j}^2\boldsymbol{\mathrm{P}}^2_{jj}}}\,\pi(u\,|\, \y)\,du.
\end{eqnarray}
For ridge regression the efcetive degrees of freedom may be calculated as the expected value trace of the matrix $\U\boldsymbol{\mathrm{P}}\U'$; that is:
\begin{eqnarray*}
edf &=& \text{E}\,\left[\text{tr}(\U\boldsymbol{\mathrm{P}}\U'\,|\,\y)\right] \,\,=\,\, \text{E}\,\left[\text{tr}(\boldsymbol{\mathrm{P}}\,|\,\y)\right]\,\, = \,\, \int_0^1 \sum_{j=1}^p \boldsymbol{\mathrm{P}}_{jj}\pi(u\,|\, \y)\, \text{d}u.
\end{eqnarray*}
A naive but useful approach to calculate the quantities above may be to plug in the posterior mode of $u$. Thus, for example, the effective degrees of freedom may be approximated by
\begin{eqnarray*}
\widehat{edf} &=& \sum_{j=1}^p \frac{\left(1-\hat{u}\right)\lambda_j}{\left(1-\hat{u}\right)\lambda_j+\hat{u}},
\end{eqnarray*}
where $\hat{u} = \text{max arg}\,\pi(u\,|\, \y)$. Obviously, when there is no shrinkage $\hat{u}=0$ and $edf=\text{rank}(\X)$.

\subsubsection{Posterior Computation through QR decomposition}
Another procedure to compute the posterior distribution is given by the QR factorization of $\X'$. This is, let $\X'=\Q\R$, where $\Q=[\Q_1,\Q_2]$ is a $p\times p$ orthonormal matrix and $\R = [\R_1',\R_2']'$ is a $p\times n$ upper triangular matrix, where the entries of the $(p-n)\times n$ matrix $\R_2$ are all zeros. Thus, proceeding in the same way as with SVD decomposition,
\begin{eqnarray}\label{eqn:sigma}
\si_{n}(u) &=& \left(\X'\X+\frac{u}{1-u}\I_p\right)^{-1} \\
&=& \left(\Q\R\R'\Q'+\frac{u}{1-u}\Q\Q'\right)^{-1} \nonumber \\
&=& \Q\left(\R\R'+\frac{u}{1-u}\I_{p}\right)^{-1}\Q' \nonumber \\
&=& \Q\begin{bmatrix} \left(\R_1\R_1'+\frac{u}{1-u}\I_n\right)^{-1} & \boldsymbol{0}_{n,p-n} \\ \boldsymbol{0}_{p-n,n} & \frac{1-u}{u}\I_{p-n}\Q' \end{bmatrix}. \nonumber
\end{eqnarray}
In the same way,
\begin{eqnarray}\label{eqn:beta}
\hat{\B}(u) &=& \si_n(u)\X'\y \\
&=& \Q\left(\R\R'+\frac{u}{1-u}\I_p\right)^{-1}\Q'\Q\R\y \nonumber \\
&=& \Q\left(\R\R'+\frac{u}{1-u}\I_p\right)^{-1}\R\y \nonumber \\
&=& \Q_1\left(\R_1\R_1'+\frac{u}{1-u}\I_n\right)^{-1}\R_1\y, \nonumber
\end{eqnarray}
and
\begin{eqnarray}
\hat{\y}(u) &=& \X\hat{\B}(u)\nonumber \\
&=& \R'\Q'\Q\left(\R\R'+\frac{u}{1-u}\I_p\right)^{-1}\R\y \\
&=& \R_1'\left(\R_1\R_1'+\frac{u}{1-u}\I_n\right)^{-1}\R_1\y. \nonumber
\end{eqnarray}
Hence, by plugging (\ref{eqn:sigma}) and (\ref{eqn:beta}) in (\ref{eq:posterior1}) is obtained another way to calculate the marginal posterior of $u$, this is,
\begin{eqnarray}
\pi(u \,|\, \y) &\propto& u^{\frac{p_0+p}{2}-1}(1-u)^{\frac{n_0+n}{2}-1} \left| (1-u)\R_1\R_1' + u\I_n \right|^{-1/2}\nonumber \\ & & \times[(1-u)(SSE(u)+n_0s_0^2)]^{-\frac{n+n_0+p_0}{2}}\left[1+\frac{u}{1-u}\frac{\hat{\boldsymbol{\beta}}'(u)\hat{\boldsymbol{\beta}}(u)+p_0d_0^2}{SSE(u)+n_0s_0^2}\right]^{-\frac{n+n_0+p_0}{2}}
\end{eqnarray}
when $n > p$. The other form, $u^{\frac{p+p_0}{2}-1}$ is replaced by $u^{\frac{n+p_0}{2}-1}$.
Note that the covariance is
\begin{eqnarray*}
\text{Cov}\left(\hat{\y}(u),\y\,|\, u\right) &=&  \sigma^2\R_1'\left(\R_1\R_1'+\frac{u}{1-u}\I_n\right)^{-1}\R_1.
\end{eqnarray*}
and the variance
\begin{eqnarray*}
\text{Var}\left(\hat{\y}(u)\,|\, u\right) &=&  \sigma^2\R_1'\left(\R_1\R_1'+\frac{u}{1-u}\I_n\right)^{-1}\R_1\left[\R_1'\left(\R_1\R_1'+\frac{u}{1-u}\I_n\right)^{-1}\R_1\right]'.
\end{eqnarray*}
Therefore it is possible to calculate the correlation as in the equation (\ref{eq:cor}). \\
In the same way the effective degrees of freedom are
\begin{eqnarray}
edf &=& \text{E}\, \left[\left. \text{tr}\left(\R_1'\left(\R_1\R_1'+\frac{u}{1-u}\I_n\right)^{-1}\R_1\right)\right | \y\right] \nonumber \\
&=& \text{E}\, \left[\left. \text{tr}\left(\R_1\R_1'\left(\R_1\R_1'+\frac{u}{1-u}\I_n\right)^{-1}\right)\right | \y\right] \nonumber \\
&=& \int_0^1  \left[\text{tr}\left(\R_1\R_1'\left(\R_1\R_1'+\frac{u}{1-u}\I_n\right)^{-1}\right)\right] \,\pi(u\,|\, \y)\,\text{d}u.
\end{eqnarray}
Therefore the ($edf$) can be approximated as,
\begin{eqnarray}
\widehat{edf} &=&  \left[\text{tr}\left(\R_1\R_1'\left(\R_1\R_1'+\frac{\hat{u}}{1-\hat{u}}\I_n\right)^{-1}\right)\right],
\end{eqnarray}
where $\hat{u} = \text{max arg}\,\pi(u\,|\, \y)$.

Since manipulating high dimensional inverse matrices is complicated and expensive, then when we want to work with the QR method, the posterior mode of u will be used to obtain the values described above. That is, instead of using a vector u, the posterior mode of u will be taken.


\newpage

\section{Results}

With this information we can build the package \pkg{HDBRR} \citep{HDBRR} in \proglang{R}, the HDBRR function is,

\begin{GrayBox}
\small
\textbf{Box 1a: List of arguments of the HDBRR function}

\begin{verbatim}
HDBRR(y, X, n0 = 5, p0 = 5, s20 = NULL, d20 = NULL, h = 0.5,
    intercept = TRUE, vpapp = TRUE,npts = NULL,c = NULL,
    corpred = NULL, method = c("svd","qr"),bigmat = TRUE, ncores = 2)
\end{verbatim}
\end{GrayBox}

also we have a second function, the matop function, this help us to compute the SVD or QR decomposition, the matop form is

\begin{GrayBox}
\small
\textbf{Box 1b: List of arguments of the matop function}

\begin{verbatim}
matop(y, X, method = c("svd", "qr"), bigmat = TRUE)
\end{verbatim}
\end{GrayBox}

\subsection{Application Example}
In this example we will use a database in the package named ``phenowheat", this contain data from a balanced, four-way multiparental cross population from four elite durum wheat cultivars (Neodur, Claudio, Colosseo, and Rascon/Tarro) that were chosen as diverse contributors of different alleles of agronomic relevance. The cultivars were crossed pair-wise following the scheme ([Neodur $\times$ Claudio] $\times$ [Colosseo $\times$ Rascon/Tarro], i.e., NCCR) to produce two-way $\text{F}_1$ NCCR hybrids. These four-way $\text{F}_1$ hybrids were advanced through single-seed descent and bulked in the $\text{F}_8$ generation. The final NCCR population includes 338 recombinant inbred lines (RILs) \citep{Milner}.This population is representative of segregating populations commonly used in wheat breeding, where the three- and four-way cross scheme is increasingly adopted to generate wider genotypic variance in segregating populations as compared with the traditional biparental cross \citep{Crossa}. \\\\ The final number of SNPs included in the NCCR linkage map was 7594. The markers were centered and standardized. Phenotypic evaluation of the NCCR population was performed during two growing seasons (2010-2011 and 2011-2012) in locations in the Po Valley representative of the target environments where durum wheat is grown: Cadriano in the 2010-2011 growin season (Cad11) and the 2011-2012 growing season (Cad12); Poggio Renatico in the 2010-2011 growing season (Pr11) and Argelato in the 2011-2012  growing season (Arg12). The 338 RILs, the four parents, and the five control genotypes were evaluated in an a-lattice incomplete-block experimental design; a 19 by 19 $\alpha$-lattice design with two replications was considered in each environment.The four traits included in this study were GY (Mg ha-1), HD (d), GWT (g 1000 kernels-1), and GVW (kg hL-1) where GY is the grain yield, HD the heading data, GWT the 1000-kernel weight and GVW the grain volume weight. The phenotypes of the four traits used for data analysis were the best linear unbiased estimates after recovering the incomplete block information (i.e., adjusting for the random incomplete block effect) in each environment and across environments \citep{Crossa}. \\\\

The next code shows the HDBRR function with this data, $y$ is obtained with the lmer function, this allows us to find the BLUP of the line with respect to the environment.

\begin{GrayBox}
\small
\textbf{Box 1c: Example with dataset "phenowheat"}

\begin{verbatim}
data("phenowheat")
mod <- lmer(pheno$HD~pheno$env+(1|pheno$Line))
y <- unlist(ranef(mod))
n <- length(y)
X <- scale(X, scale=F)
fitall <- HDBRR(y, X/sqrt(ncol(X)), intercept = FALSE, corpred = "eb", c = 100)
summary(fitall)
plot(fitall)
\end{verbatim}
\end{GrayBox}
The code in the Box 1c return the results for fitall, this shows us the coefficients obtained. If we have $p > 250$, the object shows only 250 coefficients.

\begin{GrayBox}
\small
\textbf{Box 1c: Structure of the object fitall returned by HDBRR (after running the code in Box 1c)}

\begin{verbatim}
> fitall

Call:
HDBRR(y = y, X = X/sqrt(ncol(X)), intercept = F, c = 100, corpred = "eb")

Coefficients:
       X1        X2        X3        X4        X5        X6        X7
-0.069050  0.117750 -0.075723 -0.115526 -0.006621 -0.111297 -0.062642
       X8        X9       X10       X11       X12       X13       X14
-0.088641 -0.096674 -0.114091 -0.063993 -0.082015 -0.114091 -0.107933

 ... 7580 coefficients was omitted
\end{verbatim}
\end{GrayBox}

In the same way, the object HDBRR returns a list of 21 elements, included betahat ($\hat{\B}$), yhat ($\hat{\y}$), sigsqhat ($\hat{\sigma}^2$) and sigbsqhat ($\hat{\sigma}_{\beta}^2$).

\newpage

\begin{GrayBox}
\small
\textbf{Box 1d: Structure of the object returned by HDBRR (after running the code in Box 1c)}

\begin{verbatim}
str(fm)
List of 21
 $ betahat      : num [1:7594] -0.06905 0.11775 0.07572 ...
 $ yhat         : num [1:338] -0.394 -0.52 2.876 ...
 $ sdyhat       : num [1:338] 0.643 0.648 0.619 ...
 $ sdpred       : num [1:338] 1.71 1.72 1.7 ...
 $ varb         : num [1:7594] 0.3219 0.8271 0.0511 ...
 $ sigsqhat     : num 2.52
 $ sigbsqhat    : num 6.88
 $ u            : num [1:200] 0.116 0.117 0.119 ...
 $ postu        : Named num [1:200] 0.116 0.117 0.119 ...
 ..- attr(*,"names") = chr [1:200]  "84.1344\%" ...
 $ uhat         : num 0.273
 $ umode        : num 0.261
 $ whichNa      : int(0)
 $ phat         : num [1:7594] 0.52 0.52 0.521 ...
 $ delta        : num 7.96
 $ edf          : num 140
 $ corr         : num [1:338] 0.804 0.818 0.793 ...
 $ y            : num [1:338] 0.0228 -1.5949 2.7807 ...
 $ intercept    : num 0
\end{verbatim}
\end{GrayBox}

Now, we have the option for a summary, this function returns the \texttt{estimate}, \texttt{standard deviation}, \texttt{SNR} and $2\log{BF}$ for the coefficients, in the principal function we have the argument \texttt{c}, when \texttt{c = NULL} then the summary don't have the last column (\texttt{$2\log{BF}$}). Even more, we have the arguments \texttt{all.coef} and \texttt{crit}, when we have \texttt{all.coef = TRUE} then summary returns all coefficients, but when we have \texttt{all.coef = FALSE} (the default value) summary only returns the coefficients that \texttt{log(bayes factor) > crit} where the default value for \texttt{crit} is \texttt{log{(4)}}. the summary returns the \texttt{ridge parameter} ($\lambda$) and the \texttt{effective degrees of freedom} (edf) too, when \texttt{c = NULL}, \texttt{edf = NULL}. \\ The form of the summary is in the Box 1e.

\newpage

\begin{GrayBox}
\small
\textbf{Box 1e: Summary of the object returned by HDBRR (after running the code in Box 1c) using the default values}

\begin{verbatim}
> summary(fitall)

Call:
HDBRR(y = y, X = X/sqrt(ncol(X)), intercept = F, c = 100, corpred = "eb")


Coefficients:
       Estimate  Std. dev       SNR  2ln(BF)
X1187  2.448331 0.7079706  3.458238 1.733250
X1189 -3.477229 0.5949053 -5.845014 3.105383 *
X1190 -3.411983 0.5887797 -5.795009 3.020875 *
X1191 -3.437302 0.6060903 -5.671272 3.056947 *
X1192 -3.578898 0.6424279 -5.570895 3.252233 *
X1193 -3.595580 0.6084415 -5.909492 3.273802 *
X1194 -3.548588 0.5963829 -5.950185 3.206101 *
X1195 -3.581506 0.6196334 -5.780039 3.257444 *
X1196 -3.376780 0.5930485 -5.693935 2.971416 *
X1197 -3.444590 0.6027798 -5.714507 3.063753 *
X1198 -3.443699 0.6013025 -5.727066 3.062512 *
X1199 -3.395415 0.5896655 -5.758205 2.995950 *
X1200  2.524250 0.7140561  3.535086 1.811552
X1205 -3.623269 0.6607038 -5.483955 3.312073 *
X1931 -2.484760 0.6701858 -3.707569 1.845033
X1932 -2.422271 0.6321472 -3.831815 1.774233
X1933 -2.417706 0.6402461 -3.776214 1.769846
X1934 -2.452866 0.6641450 -3.693269 1.807623
X1935 -2.120806 0.7404535 -2.864199 1.433109
X1936 -2.752589 0.7502858 -3.668721 2.168280 *
X1937 -2.437252 0.6639373 -3.670907 1.790789
X1938 -2.572225 0.6453258 -3.985932 1.946329
X1950  2.164019 0.7499884  2.885403 1.447961
-----
Signif. codes: 10 '***' 6 '**' 2 '*' 0 ' '


Ridge parameter: 0.3754
Effective degrees of freedom: 140.3917
\end{verbatim}
\end{GrayBox}


The other function implemented for a package HDBRR is plot, this returns the variable selection graphs, this use the argument \texttt{crit = log(4)} as default value. The graphs ``y observed vs y predicted", ``coefficients",``Std.dev." and ``SNR" are included too. Finally we have the graph of ``Marginal posterior of u". Then with \texttt{plot(fitall)} we obtain Figures \ref{fig:selection}, \ref{fig:multi} and \ref{fig:post}.



\begin{figure}[!htb]
  \centering
  \includegraphics[width=0.75\textwidth]{2}
  \caption{Estimate coefficients vs selection probability of $x_i$.}
  \label{fig:selection}
\end{figure}

\begin{figure}[!htb]
  \centering
  \includegraphics[width=0.75\textwidth]{3}
  \caption{y observed vs y predicted, Coefficients, td.dev. and SNR.}
  \label{fig:multi}
\end{figure}



\begin{figure}[!htb]
  \centering
  \includegraphics[width=0.75\textwidth]{4}
  \caption{Marginal posterior of u.}
  \label{fig:post}
\end{figure}

Finally we have the option \texttt{predict} for a HDBRR object, this returns the vector of predicted values.\\

One option to study the results obtained with the package is to use the cross validation, in this case we use a K-Fold cross validation with $K=10$, and we obtain the results in the Table \ref{tab:kfold}.

\begin{table}[ht]
\centering
\begin{tabular}{lrr}
  \hline
  & MSE & Correlation \\
  \hline
Fold = 1 & 4.7367 & 0.6752 \\
  Fold = 2 & 5.9848 & 0.3813 \\
  Fold = 3 & 3.533 & 0.5907 \\
  Fold = 4 & 4.772 & 0.6583 \\
  Fold = 5 & 3.9884 & 0.6657 \\
  Fold = 6 & 5.8835 & 0.5838 \\
  Fold = 7 & 3.4608 & 0.5146 \\
  Fold = 8 & 3.6765 & 0.6828 \\
  Fold = 9 & 5.1121 & 0.553 \\
  Fold = 10 & 2.7146 & 0.6892 \\
  Pooled & 4.3862 & 0.5995 \\
   \hline
\end{tabular}
\caption{K-Fold cross validation with K=10}
\label{tab:kfold}
\end{table}


\section{Conclusions}

We propose a computational method to make Bayesian inference for high-dimensional ridge regression without using MCMC methods. Posterior means and variances of regression parameters, variance components and predictions for the conventional ridge Regression model are obtained by using a convenient reparameterization. The problem is reduced to numerical integration on the open interval $(0,1)$ to get rid of a nuisance parameter, after SVD or QR decomposition of the matrix $\X^{\prime}\X$. The method is implemented in the \proglang{R} package \proglang{HDBRR}, which allows us also to make also variable selection and prediction without appealing the theoretical guarantees of MCMC methods. The results of cross validation shown that the proposed method has a performance in computation time and accuracy at least as good as the results obtained by using MCMC methods.


%At the end, the bibliography
\bibliography{references}

\end{document}
